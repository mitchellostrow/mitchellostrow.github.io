---
---

@article{delay_embed,
    bibtex_show={true},
    title= {Delay Embedding Theory of Neural Sequence Models}, 
    author={Mitchell Ostrow and Adam Eisen and Ila Fiete},
    year={2024},
    selected = {true},
    url = {https://arxiv.org/abs/2406.11993v1},
    abstract = {To generate coherent responses, language models infer unobserved meaning from their input text sequence. One potential explanation for this capability arises from theories of delay embeddings in dynamical systems, which prove that unobserved variables can be recovered from the history of only a handful of observed variables. To test whether language models are effectively constructing delay embeddings, we measure the capacities of sequence models to reconstruct unobserved dynamics. We trained 1-layer transformer decoders and state-space sequence models on next-step prediction from noisy, partially-observed time series data. We found that each sequence layer can learn a viable embedding of the underlying system. However, state-space models have a stronger inductive bias than transformers-in particular, they more effectively reconstruct unobserved information at initialization, leading to more parameter-efficient models and lower error on dynamics tasks. Our work thus forges a novel connection between dynamical systems and deep learning sequence models via delay embedding theory.}
}


@article{MPRNN,
  bibtex_show={true},
  title = {Representational Geometry of Social Inference and Generalization in a Competitive Game},
  author = {Ostrow, Mitchell, and Yang, G.R., and Seo, H.},
  year= {2022},
  abstract = {The use of an internal model to infer and predict
              others` mental states and actions, broadly referred to as Theory
              of Mind (ToM), is a fundamental aspect of human social
              intelligence. Nevertheless, it remains unknown how these models
              are used during social interactions, and how they help an agent
              generalize to new contexts. We investigated a putative neural
              mechanism of ToM in a recurrent circuit through the lens of
              an artificial neural network trained with reinforcement learning
              (RL) to play a competitive matching pennies game against
              many algorithmic opponents. The network showed near-optimal
              performance against unseen opponents, indicating that it had
              acquired the capacity to adapt against new strategies online.
              Analysis of recurrent states during play against out-of-training-distribution (OOD) 
              opponents in relation to those of within-training-distribution (WD) opponents revealed two similarity-based mechanisms by which the network might generalize:
              mapping to a known strategy (template matching) or known
              opponent category (interpolation). Even when the network’s
              strategy cannot be explained by template-matching or interpolation, the recurrent activity fell upon the low-dimensional
              manifold of the WD neural activity, suggesting the contribution of
              prior experience with WD opponents. Furthermore, these states
              occupied low-density edges of the WD-manifold, suggesting that
              the network can extrapolate beyond any learned strategy or
              category. Our results suggest that a neural implementation for
              ToM may be a reservoir of learned representations that provide
              the capacity for generalization via flexible access and reuse of
              these stored features.},
  pdf = {https://www.researchgate.net/publication/362357350_Representational_Geometry_of_Social_Inference_and_Generalization_in_a_Competitive_Game},
  video = {https://youtu.be/8hiGqHDH-3w},
  journal = {Robotics: Science and Systems },
  workshop = {Social Intelligence in Humans and Robots},
  selected = {true},
  preview = {lstm.png},
  code = {https://github.com/mitchellostrow/mprnnpublic}
}

@article{CompPsychiatry,
  title = {Examining the Viability of Computational Psychiatry: Approaches into the Future},
  author = {Ostrow, Mitchell},
  abstract = {As modern medicine becomes increasingly personalized, psychiatry lags behind, using poorly-understood drugs and therapies to treat mental disorders. With the advent of methods that capture large quantities of data, such as genome-wide analyses or fMRI, machine learning (ML) approaches have become prominent in neuroscience. This is promising for studying the brain’s function, but perhaps more importantly, these techniques can potentially predict the onset of disorder and treatment response. Experimental approaches that use naive machine learning algorithms have dominated research in computational psychiatry over the past decade. In a critical review and analysis, I argue that biologically realistic approaches will be more effective in clinical practice, and research trends should reflect this. Hybrid models are considered, and a brief case study on major depressive disorder is presented. Finally, I propose a novel four-step approach for the future implementation of computational methods in psychiatric clinics.},
  year = {2021},
  journal = {Yale Undergraduate Research Journal},
  url = {https://elischolar.library.yale.edu/yurj/vol2/iss1/35/},
  html = {https://elischolar.library.yale.edu/yurj/vol2/iss1/35/},
  volume = {2},
  number = {1}
}

@article{ostrow2023geometry,
    bibtex_show={true},
    title= {Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis}, 
    author={Mitchell Ostrow and Adam Eisen and Leo Kozachkov and Ila Fiete},
    year={2023},
    selected = {true},
    url = {https://arxiv.org/abs/2306.10168},
    preview = {torus.jpeg},
    journal = {Neural Information Processing Systems},
    code = {https://github.com/mitchellostrow/DSA},
    abstract = {How can we tell whether two neural networks are utilizing the same internal processes for a particular computation? This question is pertinent for multiple subfields of both neuroscience and machine learning, including neuroAI, mechanistic interpretability, and brain-machine interfaces. Standard approaches for comparing neural networks focus on the spatial geometry of latent states. Yet in recurrent networks, computations are implemented at the level of neural dynamics, which do not have a simple one-to-one mapping with geometry. To bridge this gap, we introduce a novel similarity metric that compares two systems at the level of their dynamics. Our method incorporates two components: Using recent advances in data-driven dynamical systems theory, we learn a high-dimensional linear system that accurately captures core features of the original nonlinear dynamics. Next, we compare these linear approximations via a novel extension of Procrustes Analysis that accounts for how vector fields change under orthogonal transformation. Via four case studies, we demonstrate that our method effectively identifies and distinguishes dynamic structure in recurrent neural networks (RNNs), whereas geometric methods fall short. We additionally show that our method can distinguish learning rules in an unsupervised manner. Our method therefore opens the door to novel data-driven analyses of the temporal structure of neural computation, and to more rigorous testing of RNNs as models of the brain.}
}

@inproceedings{explanation,
  title = {Predictive Models are not Enough for Explanation-Seeking Curiosity: A Case Study. },
  author = {Hokyung Sung and Mitchell Ostrow},
  year = {2023},
  poster = {true},
  preview = {cogsci.jpg},
  booktitle = {Curiosity, Creativity and Complexity Conference}
}

@inproceedings{MPRNNposter,
  title = {Neural Representations of Opponent Strategy Support the Adaptive Behavior of Recurrent Actor-Critics in a Competitive Game},
  author = {Ostrow, Mitchell, and Yang, G.R., and Seo, H.},
  year = {2022},
  poster = {true},
  booktitle = {Computational and Systems Neuroscience (COSYNE)},
  preview = {lstm.png}
}
@inproceedings{Michi,
  title = {Network Dimensions Alter Reversal Learning Strategies},
  author = {Naim, M., and Gibson, D., and Papageorgiou, D., and Xie, Y., Ostrow, M., Graybiel, A., and Yang, G.R},
  year = {2023},
  poster = {true},
  booktitle = {Computational and Systems Neuroscience (COSYNE)},
  preview = {lstm.png}
}

@inproceedings{CCN2023,
  title = {Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamic Similarity Analysis.},
  author = {Ostrow, M.B., and Eisen, A.J., and Kozachkov, L, and Fiete, I.R.},
  year = {2023},
  poster = {true},
  preview = {torus.jpeg},
  booktitle = {Cognitive Computational Neuroscience (CCN), selected as an oral presentation (5\%)},
}


@inproceedings{COSYNE2023,
  title = {Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamic Similarity Analysis.},
  author = {Ostrow, M.B., and Eisen, A.J., and Kozachkov, L, and Fiete, I.R.},
  year = {2023},
  poster = {true},
  preview = {torus.jpeg},
  booktitle = {Computational Systems Neuroscience Conference (COSYNE), selected as an oral presentation (2\%)},
}



@misc{concepts,
title = {Do Deep Neural Networks Have Concepts?},
author = {Mitchell Ostrow, and Tony Chen, and Cedegao Zhang, and Hokyung Sung},
year = {2023},
poster = {true},
preview = {cogsci.jpg},
howpublished = {Philosophy of Deep Learning Conference}
}

@misc{ibl,
title = {Investigating the Interplay of Anatomical, Biophysical, and Functional Modularity in Task-Optimized RNNs},
author = {Mitchell Ostrow, and Hokyung Sung, and Ila Fiete},
year = {2023},
howpublished = {IBL U19 Conference}
}

@misc{MPRNN,
  bibtex_show={true},
<<<<<<< HEAD
  title = {Representational Geometry of Social Inference and Generalization in a Competitive Game},
  author = {Ostrow, Mitchell, and Yang, G.R., and Seo, H.},
  year= {2022},
  howpublished = {Robotics: Science and Systems (Spotlight Talk)},
  preview = {lstm.png},
=======
  title={The meaning of relativity},
  author={Einstein, Albert and Taub, AH},
  journal={American Journal of Physics},
  volume={18},
  number={6},
  pages={403--404},
  year={1950},
  publisher={American Association of Physics Teachers}
}

@article{PhysRev.47.777,
  abbr={PhysRev},
  title={Can Quantum-Mechanical Description of Physical Reality Be Considered Complete?},
  author={Einstein*†, A. and Podolsky*, B. and Rosen*, N.},
  abstract={In a complete theory there is an element corresponding to each element of reality. A sufficient condition for the reality of a physical quantity is the possibility of predicting it with certainty, without disturbing the system. In quantum mechanics in the case of two physical quantities described by non-commuting operators, the knowledge of one precludes the knowledge of the other. Then either (1) the description of reality given by the wave function in quantum mechanics is not complete or (2) these two quantities cannot have simultaneous reality. Consideration of the problem of making predictions concerning a system on the basis of measurements made on another system that had previously interacted with it leads to the result that if (1) is false then (2) is also false. One is thus led to conclude that the description of reality as given by a wave function is not complete.},
  journal={Phys. Rev.},
  location={New Jersey},
  volume={47},
  issue={10},
  pages={777--780},
  numpages={0},
  year={1935},
  month={May},
  publisher=aps,
  doi={10.1103/PhysRev.47.777},
  url={http://link.aps.org/doi/10.1103/PhysRev.47.777},
  html={https://journals.aps.org/pr/abstract/10.1103/PhysRev.47.777},
  pdf={example_pdf.pdf},
  altmetric={248277},
  dimensions={true},
  google_scholar_id={qyhmnyLat1gC},
  video={https://www.youtube-nocookie.com/embed/aqz-KE-bpKQ},
  additional_info={. *More Information* can be [found here](https://github.com/alshedivat/al-folio/)},
  annotation={* Example use of superscripts<br>† Albert Einstein},
  selected={true},
  inspirehep_id = {3255}
}

@article{einstein1905molekularkinetischen,
  title={{\"U}ber die von der molekularkinetischen Theorie der W{\"a}rme geforderte Bewegung von in ruhenden Fl{\"u}ssigkeiten suspendierten Teilchen},
  author={Einstein, A.},
  journal={Annalen der physik},
  volume={322},
  number={8},
  pages={549--560},
  year={1905},
  publisher={Wiley Online Library}
}

@article{einstein1905movement,
  abbr={Ann. Phys.},
  title={Un the movement of small particles suspended in statiunary liquids required by the molecular-kinetic theory 0f heat},
  author={Einstein, A.},
  journal={Ann. Phys.},
  volume={17},
  pages={549--560},
  year={1905}
}

@article{einstein1905electrodynamics,
  title={On the electrodynamics of moving bodies},
  author={Einstein, A.},
  year={1905}
}

@Article{einstein1905photoelectriceffect,
  bibtex_show={true},
  abbr={Ann. Phys.},
  title="{{\"U}ber einen die Erzeugung und Verwandlung des Lichtes betreffenden heuristischen Gesichtspunkt}",
  author={Albert Einstein},
  abstract={This is the abstract text.},
  journal={Ann. Phys.},
  volume={322},
  number={6},
  pages={132--148},
  year={1905},
  doi={10.1002/andp.19053220607},
  award={Albert Einstein receveid the **Nobel Prize in Physics** 1921 *for his services to Theoretical Physics, and especially for his discovery of the law of the photoelectric effect*},
  award_name={Nobel Prize}
}

@book{przibram1967letters,
  bibtex_show={true},
  title={Letters on wave mechanics},
  author={Einstein, Albert and Schrödinger, Erwin and Planck, Max and Lorentz, Hendrik Antoon and Przibram, Karl},
  year={1967},
  publisher={Vision},
  preview={wave-mechanics.gif},
  abbr={Vision}
>>>>>>> upstream/main
}
