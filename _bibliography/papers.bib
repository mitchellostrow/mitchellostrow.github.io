---
---

@article{MPRNN,
  bibtex_show={true},
  title = {Representational Geometry of Social Inference and Generalization in a Competitive Game},
  author = {Ostrow, Mitchell, and Yang, G.R., and Seo, H.},
  year= {2022},
  abstract = {The use of an internal model to infer and predict
              others` mental states and actions, broadly referred to as Theory
              of Mind (ToM), is a fundamental aspect of human social
              intelligence. Nevertheless, it remains unknown how these models
              are used during social interactions, and how they help an agent
              generalize to new contexts. We investigated a putative neural
              mechanism of ToM in a recurrent circuit through the lens of
              an artificial neural network trained with reinforcement learning
              (RL) to play a competitive matching pennies game against
              many algorithmic opponents. The network showed near-optimal
              performance against unseen opponents, indicating that it had
              acquired the capacity to adapt against new strategies online.
              Analysis of recurrent states during play against out-of-training-distribution (OOD) 
              opponents in relation to those of within-training-distribution (WD) opponents revealed two similarity-based mechanisms by which the network might generalize:
              mapping to a known strategy (template matching) or known
              opponent category (interpolation). Even when the network’s
              strategy cannot be explained by template-matching or interpolation, the recurrent activity fell upon the low-dimensional
              manifold of the WD neural activity, suggesting the contribution of
              prior experience with WD opponents. Furthermore, these states
              occupied low-density edges of the WD-manifold, suggesting that
              the network can extrapolate beyond any learned strategy or
              category. Our results suggest that a neural implementation for
              ToM may be a reservoir of learned representations that provide
              the capacity for generalization via flexible access and reuse of
              these stored features.},
  url = {https://social-intelligence-human-ai.github.io/docs/camready_8.pdf},
  pdf = {https://social-intelligence-human-ai.github.io/docs/camready_8.pdf},
  video = {https://youtu.be/8hiGqHDH-3w},
  journal = {Robotics: Science and Systems },
  workshop = {Social Intelligence in Humans and Robots},
  selected = {true},
  preview = {lstm.png},
  code = {https://github.com/mitchellostrow/mprnnpublic}
}

@article{CompPsychiatry,
  title = {Examining the Viability of Computational Psychiatry: Approaches into the Future},
  author = {Ostrow, Mitchell},
  abstract = {As modern medicine becomes increasingly personalized, psychiatry lags behind, using poorly-understood drugs and therapies to treat mental disorders. With the advent of methods that capture large quantities of data, such as genome-wide analyses or fMRI, machine learning (ML) approaches have become prominent in neuroscience. This is promising for studying the brain’s function, but perhaps more importantly, these techniques can potentially predict the onset of disorder and treatment response. Experimental approaches that use naive machine learning algorithms have dominated research in computational psychiatry over the past decade. In a critical review and analysis, I argue that biologically realistic approaches will be more effective in clinical practice, and research trends should reflect this. Hybrid models are considered, and a brief case study on major depressive disorder is presented. Finally, I propose a novel four-step approach for the future implementation of computational methods in psychiatric clinics.},
  year = {2021},
  journal = {Yale Undergraduate Research Journal},
  url = {https://elischolar.library.yale.edu/yurj/vol2/iss1/35/},
  html = {https://elischolar.library.yale.edu/yurj/vol2/iss1/35/},
  volume = {2},
  number = {1}
}

@article{ostrow2023geometry,
    bibtex_show={true},
    title= {Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis}, 
    author={Mitchell Ostrow and Adam Eisen and Leo Kozachkov and Ila Fiete},
    year={2023},
    selected = {true},
    url = {https://arxiv.org/abs/2306.10168},
    preview = {torus.jpeg},
    journal = {ArXiv},
    code = {https://github.com/mitchellostrow/DSA},
    abstract = {How can we tell whether two neural networks are utilizing the same internal processes for a particular computation? This question is pertinent for multiple subfields of both neuroscience and machine learning, including neuroAI, mechanistic interpretability, and brain-machine interfaces. Standard approaches for comparing neural networks focus on the spatial geometry of latent states. Yet in recurrent networks, computations are implemented at the level of neural dynamics, which do not have a simple one-to-one mapping with geometry. To bridge this gap, we introduce a novel similarity metric that compares two systems at the level of their dynamics. Our method incorporates two components: Using recent advances in data-driven dynamical systems theory, we learn a high-dimensional linear system that accurately captures core features of the original nonlinear dynamics. Next, we compare these linear approximations via a novel extension of Procrustes Analysis that accounts for how vector fields change under orthogonal transformation. Via four case studies, we demonstrate that our method effectively identifies and distinguishes dynamic structure in recurrent neural networks (RNNs), whereas geometric methods fall short. We additionally show that our method can distinguish learning rules in an unsupervised manner. Our method therefore opens the door to novel data-driven analyses of the temporal structure of neural computation, and to more rigorous testing of RNNs as models of the brain.}
}

@inproceedings{explanation,
  title = {Predictive Models are not Enough for Explanation-Seeking Curiosity: A Case Study. },
  author = {Hokyung Sung and Mitchell Ostrow},
  year = {2023},
  preview = {cogsci.jpg},
  booktitle = {Curiosity, Creativity and Complexity Conference}
}

@inproceedings{MPRNNposter,
  title = {Neural Representations of Opponent Strategy Support the Adaptive Behavior of Recurrent Actor-Critics in a Competitive Game},
  author = {Ostrow, Mitchell, and Yang, G.R., and Seo, H.},
  year = {2022},
  booktitle = {Computational and Systems Neuroscience (COSYNE)},
  preview = {lstm.png}
}
@inproceedings{Michi,
  title = {Network Dimensions Alter Reversal Learning Strategies},
  author = {Naim, M., and Gibson, D., and Papageorgiou, D., and Xie, Y., Ostrow, M., Graybiel, A., and Yang, G.R},
  year = {2023},
  booktitle = {Computational and Systems Neuroscience (COSYNE)},
  preview = {lstm.png}
}

@inproceedings{CCN2023,
  title = {Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamic Similarity Analysis.},
  author = {Ostrow, M.B., and Eisen, A.J., and Kozachkov, L, and Fiete, I.R.},
  year = {2023},
  selected = {true},
  preview = {torus.jpeg},
  booktitle = {Cognitive Computational Neuroscience (CCN) selected as an oral presentation},
}

@misc{concepts,
title = {Do Deep Neural Networks Have Concepts?},
author = {Mitchell Ostrow, and Tony Chen, and Cedegao Zhang, and Hokyung Sung},
year = {2023},
preview = {cogsci.jpg},
howpublished = {Philosophy of Deep Learning Conference}
}

@misc{ibl,
title = {Investigating the Interplay of Anatomical, Biophysical, and Functional Modularity in Task-Optimized RNNs},
author = {Mitchell Ostrow, and Hokyung Sung, and Ila Fiete},
year = {2023},
howpublished = {IBL U19 Conference}
}

@misc{MPRNN,
  bibtex_show={true},
  title = {Representational Geometry of Social Inference and Generalization in a Competitive Game},
  author = {Ostrow, Mitchell, and Yang, G.R., and Seo, H.},
  year= {2022},
  howpublished = {Robotics: Science and Systems (Spotlight Talk)},
  preview = {lstm.png},
}