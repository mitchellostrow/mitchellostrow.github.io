@article{eisen2025characterizingcontrolinteractingsubsystems,
  title={Characterizing control between interacting subsystems with deep Jacobian estimation},
  author={Adam J. Eisen and Mitchell Ostrow and Sarthak Chandra and Leo Kozachkov and Earl K. Miller and Ila R. Fiete},
  year={2025},
  preview={jacobian.png},
  journal={arxiv},
  selected={true},
  url={https://www.biorxiv.org/content/10.1101/2025.07.01.637062v1},
  topic={ML methods for neural data analysis}
}

@article{liang2024diffusionmodelslearnfactorize,
  title={How Diffusion Models Learn to Factorize and Compose},
  author={Qiyao Liang and Ziming Liu and Mitchell Ostrow and Ila Fiete},
  year={2024},
  preview={horseastronaut.jpg},
  journal={Neural Information Processing Systems},
  selected={true},
  url={https://arxiv.org/abs/2408.13256},
  topic={Understanding neural systems}
}

@article{schaeffer2024does,
  title={Does Maximizing Neural Regression Scores Teach Us About The Brain?},
  author={Rylan Schaeffer and Mikail Khona and Sarthak Chandra and Mitchell Ostrow and Brando Miranda and Sanmi Koyejo},
  journal={UniReps: 2nd Edition of the Workshop on Unifying Representations in Neural Models},
  year={2024},
  url={https://openreview.net/forum?id=vbtj05J68r},
  topic={Understanding neural systems}
}

@article{Versteeg2025,
  author={Versteeg, Christopher and McCart, Jonathan D. and Ostrow, Mitchell and Zoltowski, David M. and Washington, Clayton B. and Driscoll, Laura and Codol, Olivier and Michaels, Jonathan A. and Linderman, Scott W. and Sussillo, David and Pandarinath, Chethan},
  title={Computation-through-Dynamics Benchmark: Simulated datasets and quality metrics for dynamical models of neural activity},
  elocation-id={2025.02.07.637062},
  year={2025},
  doi={10.1101/2025.02.07.637062},
  publisher={Cold Spring Harbor Laboratory},
  abstract={A primary goal of systems neuroscience is to discover how ensembles of neurons transform inputs into goal-directed behavior, a process known as neural computation. A powerful framework for understanding neural computation uses neural dynamics {\textendash} the rules that describe the temporal evolution of neural activity {\textendash} to explain how goal-directed input-output transformations occur. As dynamical rules are not directly observable, we need computational models that can infer neural dynamics from recorded neural activity. We typically validate such models using synthetic datasets with known ground-truth dynamics, but unfortunately existing synthetic datasets don{\textquoteright}t reflect fundamental features of neural computation and are thus poor proxies for neural systems. Further, the field lacks validated metrics for quantifying the accuracy of the dynamics inferred by models. The Computation-through-Dynamics Benchmark (CtDB) fills these critical gaps by providing: 1) synthetic datasets that reflect computational properties of biological neural circuits, 2) interpretable metrics for quantifying model performance, and 3) a standardized pipeline for training and evaluating models with or without known external inputs. In this manuscript, we demonstrate how CtDB can help guide the development, tuning, and troubleshooting of neural dynamics models. In summary, CtDB provides a critical platform for model developers to better understand and characterize neural computation through the lens of dynamics.Competing Interest StatementD.S. and C.P. are employees of Meta Platforms Reality Labs. Meta Platforms Reality Labs did not support this work, nor did it have any role in the study or any competing interests related to it.},
  URL={https://www.biorxiv.org/content/early/2025/02/20/2025.02.07.637062},
  journal={bioRxiv},
  topic={ML methods for neural data analysis}
}

@article{delay_embed,
  bibtex_show={true},
  title={Delay Embedding Theory of Neural Sequence Models},
  author={Mitchell Ostrow and Adam Eisen and Ila Fiete},
  year={2024},
  selected={true},
  preview={delayembed.png},
  journal={International Conference on Machine Learning (ICML) workshop on Next Generation Sequence Modeling},
  url={https://arxiv.org/abs/2406.11993v1},
  abstract={To generate coherent responses, language models infer unobserved meaning from their input text sequence. One potential explanation for this capability arises from theories of delay embeddings in dynamical systems, which prove that unobserved variables can be recovered from the history of only a handful of observed variables. To test whether language models are effectively constructing delay embeddings, we measure the capacities of sequence models to reconstruct unobserved dynamics. We trained 1-layer transformer decoders and state-space sequence models on next-step prediction from noisy, partially-observed time series data. We found that each sequence layer can learn a viable embedding of the underlying system. However, state-space models have a stronger inductive bias than transformers-in particular, they more effectively reconstruct unobserved information at initialization, leading to more parameter-efficient models and lower error on dynamics tasks. Our work thus forges a novel connection between dynamical systems and deep learning sequence models via delay embedding theory.},
  topic={Understanding neural systems}
}

@article{MPRNN_article,
  bibtex_show={true},
  title={Representational Geometry of Social Inference and Generalization in a Competitive Game},
  author={Ostrow, Mitchell and Yang, G.R. and Seo, H.},
  year={2022},
  abstract={The use of an internal model to infer and predict others` mental states and actions, broadly referred to as Theory of Mind (ToM), is a fundamental aspect of human social intelligence. Nevertheless, it remains unknown how these models are used during social interactions, and how they help an agent generalize to new contexts. We investigated a putative neural mechanism of ToM in a recurrent circuit through the lens of an artificial neural network trained with reinforcement learning (RL) to play a competitive matching pennies game against many algorithmic opponents. The network showed near-optimal performance against unseen opponents, indicating that it had acquired the capacity to adapt against new strategies online. Analysis of recurrent states during play against out-of-training-distribution (OOD) opponents in relation to those of within-training-distribution (WD) opponents revealed two similarity-based mechanisms by which the network might generalize: mapping to a known strategy (template matching) or known opponent category (interpolation). Even when the network’s strategy cannot be explained by template-matching or interpolation, the recurrent activity fell upon the low-dimensional manifold of the WD neural activity, suggesting the contribution of prior experience with WD opponents. Furthermore, these states occupied low-density edges of the WD-manifold, suggesting that the network can extrapolate beyond any learned strategy or category. Our results suggest that a neural implementation for ToM may be a reservoir of learned representations that provide the capacity for generalization via flexible access and reuse of these stored features.},
  pdf={https://www.researchgate.net/publication/362357350_Representational_Geometry_of_Social_Inference_and_Generalization_in_a_Competitive_Game},
  video={https://youtu.be/8hiGqHDH-3w},
  journal={Robotics: Science and Systems},
  workshop={Social Intelligence in Humans and Robots},
  selected={true},
  preview={lstm.png},
  code={https://github.com/mitchellostrow/mprnnpublic},
  topic={Understanding neural systems}
}

@article{CompPsychiatry,
  title={Examining the Viability of Computational Psychiatry: Approaches into the Future},
  author={Ostrow, Mitchell},
  abstract={As modern medicine becomes increasingly personalized, psychiatry lags behind, using poorly-understood drugs and therapies to treat mental disorders. With the advent of methods that capture large quantities of data, such as genome-wide analyses or fMRI, machine learning (ML) approaches have become prominent in neuroscience. This is promising for studying the brain’s function, but perhaps more importantly, these techniques can potentially predict the onset of disorder and treatment response. Experimental approaches that use naive machine learning algorithms have dominated research in computational psychiatry over the past decade. In a critical review and analysis, I argue that biologically realistic approaches will be more effective in clinical practice, and research trends should reflect this. Hybrid models are considered, and a brief case study on major depressive disorder is presented. Finally, I propose a novel four-step approach for the future implementation of computational methods in psychiatric clinics.},
  year={2021},
  journal={Yale Undergraduate Research Journal},
  url={https://elischolar.library.yale.edu/yurj/vol2/iss1/35/},
  html={https://elischolar.library.yale.edu/yurj/vol2/iss1/35/},
  volume={2},
  number={1},
  topic={Understanding neural systems}
}

@article{ostrow2023geometry,
  bibtex_show={true},
  title={Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis},
  author={Mitchell Ostrow and Adam Eisen and Leo Kozachkov and Ila Fiete},
  year={2023},
  selected={true},
  url={https://arxiv.org/abs/2306.10168},
  preview={torus.jpeg},
  journal={Neural Information Processing Systems},
  code={https://github.com/mitchellostrow/DSA},
  abstract={How can we tell whether two neural networks are utilizing the same internal processes for a particular computation? This question is pertinent for multiple subfields of both neuroscience and machine learning, including neuroAI, mechanistic interpretability, and brain-machine interfaces. Standard approaches for comparing neural networks focus on the spatial geometry of latent states. Yet in recurrent networks, computations are implemented at the level of neural dynamics, which do not have a simple one-to-one mapping with geometry. To bridge this gap, we introduce a novel similarity metric that compares two systems at the level of their dynamics. Our method incorporates two components: Using recent advances in data-driven dynamical systems theory, we learn a high-dimensional linear system that accurately captures core features of the original nonlinear dynamics. Next, we compare these linear approximations via a novel extension of Procrustes Analysis that accounts for how vector fields change under orthogonal transformation. Via four case studies, we demonstrate that our method effectively identifies and distinguishes dynamic structure in recurrent neural networks (RNNs), whereas geometric methods fall short. We additionally show that our method can distinguish learning rules in an unsupervised manner. Our method therefore opens the door to novel data-driven analyses of the temporal structure of neural computation, and to more rigorous testing of RNNs as models of the brain.},
  topic={ML methods for neural data analysis}
}

@inproceedings{explanation,
  title={Predictive Models are not Enough for Explanation-Seeking Curiosity: A Case Study.},
  author={Hokyung Sung and Mitchell Ostrow},
  year={2023},
  poster={true},
  preview={cogsci.jpg},
  booktitle={Curiosity, Creativity and Complexity Conference},
  topic={Understanding neural systems}
}

@inproceedings{MPRNNposter,
  title={Neural Representations of Opponent Strategy Support the Adaptive Behavior of Recurrent Actor-Critics in a Competitive Game},
  author={Ostrow, Mitchell and Yang, G.R. and Seo, H.},
  year={2022},
  poster={true},
  booktitle={Computational and Systems Neuroscience (COSYNE)},
  preview={lstm.png},
  topic={Understanding neural systems}
}

@inproceedings{Michi,
  title={Network Dimensions Alter Reversal Learning Strategies},
  author={Naim, M. and Gibson, D. and Papageorgiou, D. and Xie, Y. and Ostrow, M. and Graybiel, A. and Yang, G.R},
  year={2023},
  poster={true},
  booktitle={Computational and Systems Neuroscience (COSYNE)},
  preview={lstm.png},
  topic={Understanding neural systems}
}

@inproceedings{CCN2023,
  title={Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamic Similarity Analysis.},
  author={Ostrow, M.B. and Eisen, A.J. and Kozachkov, L. and Fiete, I.R.},
  year={2023},
  poster={true},
  preview={torus.jpeg},
  booktitle={Cognitive Computational Neuroscience (CCN), selected as an oral presentation (5\%)},
  topic={ML methods for neural data analysis}
}

@inproceedings{COSYNE2023,
  title={Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamic Similarity Analysis.},
  author={Ostrow, M.B. and Eisen, A.J. and Kozachkov, L. and Fiete, I.R.},
  year={2023},
  poster={true},
  preview={torus.jpeg},
  booktitle={Computational Systems Neuroscience Conference (COSYNE), selected as an oral presentation (2\%)},
  topic={ML methods for neural data analysis}
}

@misc{concepts,
  title={Do Deep Neural Networks Have Concepts?},
  author={Mitchell Ostrow and Tony Chen and Cedegao Zhang and Hokyung Sung},
  year={2023},
  poster={true},
  preview={cogsci.jpg},
  howpublished={Philosophy of Deep Learning Conference},
  topic={Understanding neural systems}
}

@misc{ibl,
  title={Investigating the Interplay of Anatomical, Biophysical, and Functional Modularity in Task-Optimized RNNs},
  author={Mitchell Ostrow and Hokyung Sung and Ila Fiete},
  year={2023},
  howpublished={IBL U19 Conference},
  topic={Understanding neural systems}
}

@misc{MPRNN_misc,
  bibtex_show={true},
  title={Representational Geometry of Social Inference and Generalization in a Competitive Game},
  author={Ostrow, Mitchell and Yang, G.R. and Seo, H.},
  year={2022},
  howpublished={Robotics: Science and Systems (Spotlight Talk)},
  preview={lstm.png},
  topic={Understanding neural systems}
}
